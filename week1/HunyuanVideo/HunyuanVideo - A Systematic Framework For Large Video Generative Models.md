为了增强公共社区的能力,首个开源的基础视频生成模型

| ![[Pasted image 20250621124912.png]] | ![[Pasted image 20250621124916.png]] |
| ------------------------------------ | ------------------------------------ |

## Data Pre-processing

### Data Filtering
视频被精心分为五个不同的组，而图像被分为两组


1. 用PySceneDetect和OpenCV的Laplacian算子拆视频变成多个视频剪辑样本序列
2. 使用自己训练的VideoCLIP将视频剪辑样本序列进行嵌入编码
	1. 计算余弦相似度删除相似剪辑序列
	2. 使用k-means将剪辑序列进行类别聚类(10k)
3. 用一系列的fliter对数据进行过滤
	1. Dover 美学/技术评估
	2. estimated optical flow 光流预测剪辑的运动速度(过滤静态/慢动作)
	3. OCR删除包含过多文本的剪辑,裁剪视频字母
	4. 类YOLOX视觉模型删除遮挡/敏感信息剪辑
产生5个视频训练数据集,对应五个训练阶段,均是通过逐步提高过滤器的阈值来策划的

通过重用大多数的过滤器(不包括运动相关)来建立图像数据集,建立了两个,一个十亿用来做text to image预训练的初始阶段,另一个数亿用来text to image预训练的第二阶段

### Data Annotation
#### Structured Captioning
使用内部开发的视觉语言模型(VLM)构建结构化的json文本提示数据
#### Camera Movement Types
训练一个相机移动类型的分类器,将高置信度的移动类型集成到json数据中

### Model Architecture Design
![[Pasted image 20250621130353.png]]

整体架构
首先将视频剪辑/图像输入3DVAE Encoder进行编码降维得到潜在时空特征,加入噪声
然后将前面的Structured Captioning得到的json经过llm编码成一维特征向量
将前面加噪声后的图像的潜在时空特征经过flaten展平后与文本特征拼接输入diffusion backbone进行重构得到重构后的潜在时空特征,最后经过3DVAE Decoder解码得到重构后的视频剪辑/图像

#### 3D Variational Auto-encoder Design
![[Pasted image 20250621131509.png]]
高维数据映射到低维潜在空间的概率生成模型
- **编码器**：
    - 使用带因果性的3D卷积（CausalConv3D）：
        - “因果”是为了保证视频时间序列的顺序性，避免未来帧信息泄露给过去帧。
    - 输入尺寸 $(T+1) \times H \times W$，输出经过空间和时间压缩成 $\left(\frac{T}{4}+1\right) \times \frac{H}{8} \times \frac{W}{8}$ 以及 $C=16$通道的潜在特征。
- **解码器**：
    - 类似逆卷积结构，输入潜在表示还原回原始视频尺寸。
- 这种结构保证视频时空特征都被编码进潜在空间。

##### VAE 
- 传统VAE的核心是最大化数据的边缘似然 ，使用变分推断学习潜在变量 的分布：
$\log p(x) \geq \underbrace{\mathbb{E}_{q(z|x)}[\log p(x|z)]}_{\text{重构数据:拟合数据特征}} - \underbrace{\text{KL}(q(z|x) \| p(z))}_{\text{KL散度:接近先验分布}}$
 在这里：
- $x$是视频帧序列，
- $z$是潜在空间特征（encoder输出），
- $q(z|x)$是编码器给出的近似后验，    
- $p(x|z)$是解码器重构的似然，
- $p(z)$是先验（一般标准正态）。

训练时模型优化重构误差和潜在空间分布的KL散度平衡，达到既能还原视频又能学到有结构的潜在空间。

这个潜在空间特征随后被扩散模型用来做生成任务。

#### **总损失函数**
$Loss=L_1​+0.1L_{lpips}​+0.05L_{adv}​+10^{−6}L_{kl}​$
##### **各损失项核心作用**
1. **$L_1$损失（像素级重建损失）**
    - **目标**：计算生成视频 / 图像与原始数据的像素绝对误差，确保基础内容还原（如物体轮廓、颜色分布）。
    - **举例**：让生成的猫脸轮廓与原图基本一致，避免严重变形。
2. **感知损失（$L_lpips$）**
    - **目标**：通过预训练网络提取高层语义特征（如纹理、姿态），提升视觉 “真实感”（人类感知层面的相似性）。
    - **举例**：确保生成的猫毛纹理自然，而非模糊一片。
3. **对抗损失（$L_adv$）**
    - **目标**：通过生成对抗网络（GAN）让模型输出更符合真实数据分布，增强细节真实性（如光影、动态模糊）。
    - **举例**：让猫在阳光下的影子效果更自然，接近真实照片。
4. **KL 散度损失（$L_kl$）**
    - **目标**：约束潜在空间特征分布接近标准正态分布，保证生成结果的泛化性和可编辑性（如潜在空间插值平滑）。
    - **举例**：避免模型只记住训练集中的特定猫，而能生成不同姿态、毛色的猫。
##### **权重设计逻辑**
- **L₁权重最大（1.0）**：优先保证基础重建质量，避免内容丢失。
- **Llpips（0.1）和 Ladv（0.05）权重较低**：辅助优化细节，防止过度追求真实感导致训练不稳定。
- **Lkl 权重极小（10⁻⁶）**：弱约束潜在空间，避免影响重建精度。
##### **总结*image*
该损失函数通过 “像素还原 - 语义增强 - 真实感优化 - 潜在空间约束” 的多层机制，平衡视频生成的准确性、视觉质量和模型稳定性，适用于高动态视频的重建与生成任务。


为了提高高动态视频的重建(计算量倍增,动态难以捕捉)，我们从范围内 1∼8 随机选择一个采样间隔，在视频剪辑中均匀采样帧。

#### 时空平铺策略
沿空间和时间维度将输入视频分割成重叠的图块。每个图块都单独编码/解码，输出拼接在一起。对于重叠区域，我们采用线性组合进行混合。这种平铺策略允许我们在单个 GPU 上以任意分辨率和持续时间对视频进行编码/解码。
由于训练和推理之间的不一致，在推理过程中直接使用平铺策略可能会导致可见的伪影。为了解决这个问题，引入dropout,随机禁用/启用平铺,这确保了模型与平铺和非平铺策略兼容，从而保持训练和推理之间的一致性。

#### Unified Image and Video Generative Architecture
![[Pasted image 20250621145619.png]]
Caption:用户输入的提示词
经过CLIP和多种MLLM处理

CLIP编码得到的特征向量和时间步得到的特征向量相加得到y与后面得到的文本/视频token拼接

MLLM(Decoder-only)模型对用户的提示词进行编码,可以有更好的语义对齐(相比于T5这种Encoder-Decoder架构),但是因为他是Causal Attention,单向语义,对于后面的文本没有怎么关注,而图像需要整体的语义,所以我们又需要经过一个Refiner模块(进行bidirectional attention)双向注意力,强化文本token之间的关联,最后作为diffusion backbone的文本token输入部分
![[Pasted image 20250621150010.png]]

Noisy Input是前面得到的加噪声的视频潜在特征,经过patchify(也就是前面的平铺)输入backbone作为视频/图像token和前面得到的文本token拼接

接下来是双流到单流的模型设计,在双流阶段，视频和文本标记通过多个 Transformer 模块独立处理，使每种模态都能在没有干扰的情况下学习自己适当的调制机制。在单流阶段，我们将视频和文本标记连接起来，并将它们馈送到后续的 Transformer 模块中，以实现有效的多模态信息融合。这种设计捕获了视觉和语义信息之间的复杂交互，提高了模型的整体性能。

为了支持多分辨率、多纵横比和不同的持续时间生成，我们在每个 Transformer 模块中使用旋转位置嵌入 （RoPE:相对位置语义）。RoPE 将旋转频率矩阵应用于嵌入，增强了模型捕获绝对和相对位置关系的能力

GATE模块是一个权重控制门,控制信息的流动与融合
$output=Gate⊙input+(1−Gate)⊙other\_input$
- 比如在双流→单流的过渡中，Gate 可动态调节 “视频特征和文本特征的融合比例”，让模型根据任务需求（如更侧重视觉细节或文本语义 ），自适应决定信息的权重分配，增强模型对复杂多模态交互的处理能力。

Full Attention
与分割的时空注意力相比，它表现出了卓越的性能,其次，它支持图像和视频的统一生成，简化了训练过程，提高了模型的可扩展性。

在探究模型缩放规律时，作者先构建DiT - T2X模型家族，涵盖从92M到6.6B不同参数规模的图像版（T2X(I) ）与视频版（T2X(V) ）模型，以统一超参数、数据集训练，控制变量保证公平。训练中记录各模型计算量与训练损失关系绘制曲线，提取不同计算量下最低损失点连成包络线，代表给定计算资源时理论最优模型表现。基于包络线数据，用数学拟合推导模型参数（N）、数据集大小（D）与计算量（C）的幂律公式$N_{\text{opt}} = a_1 \cdot C^{b_1}$、$D_{\text{opt}} = a_2 \cdot C^{b_2}$ ，通过统计方法求解系数并验证拟合效果。对于视频模型，复用图像模型成果，选图像模型最优checkpoint初始化视频模型再训练，重复流程得到视频模型缩放规律，实现以“控制变量实验→提取最优数据→数学拟合规律”闭环，用规律指导模型训练资源分配 。

**同尺寸（分辨率、时长）的适配**
**多分辨率编码：Patchify 与 Unpatchify**
	无论输入视频 / 图像的原始尺寸如何（如 256px、512px、960px ），先将其**分块处理**（类似 ViT 的 Patch 划分 ）
**动态位置编码：3D RoPE**
	**归一化坐标**替代绝对像素 / 帧位置(相对位置)
**双流 / 单流 DiT Block：灵活处理 Token 序列**
	**不依赖固定长度的 Token 序列**

#### Model-pretraining
HunyuanVideo的模型预训练采用Flow Matching框架，将训练分多阶段进行。先运用该框架，通过对概率密度函数的变量变换与逆变换实现数据生成，训练时构建含噪声和目标潜在表示的样本，让模型预测引导样本变换的速度并优化参数。在图像预训练阶段，采用两阶段渐进策略，先以256px低分辨率图像预训练，开展多纵横比训练并学习低频概念，再进行256px与512px的混合规模训练，动态调整批次大小，维持低分辨率能力的同时学习高分辨率。视频 - 图像联合训练阶段，先按时长和纵横比对视频数据分组，训练时随机选组优化资源利用；再通过渐进式课程学习，从低分辨率短视频起步，逐步过渡到低分辨率长视频、高分辨率长视频阶段，各阶段融入不同比例图像，解决视频数据稀缺问题，防止模型遗忘图像空间语义，以此提升视频生成性能与模型泛化能力。


#### Prompt Rewrite
通过混元大模型对用户的prompt进行结构化重构,使用LoRA对Hunyuan-Large进行微调

